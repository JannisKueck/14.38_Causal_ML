{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010003,
     "end_time": "2021-03-28T13:27:45.376868",
     "exception": false,
     "start_time": "2021-03-28T13:27:45.366865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "This notebook contains an example for teaching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "051d70d956493feee0c6d64651c6a088724dca2a",
    "papermill": {
     "duration": 0.008782,
     "end_time": "2021-03-28T13:27:45.395504",
     "exception": false,
     "start_time": "2021-03-28T13:27:45.386722",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A Simple Case Study using Wage Data from 2015 - proceeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008735,
     "end_time": "2021-03-28T13:27:45.413302",
     "exception": false,
     "start_time": "2021-03-28T13:27:45.404567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So far we considered many machine learning method, e.g Lasso and Random Forests, to build a predictive model. In this lab, we extend our toolbox by predicting wages by a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009256,
     "end_time": "2021-03-28T13:27:45.431663",
     "exception": false,
     "start_time": "2021-03-28T13:27:45.422407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0088,
     "end_time": "2021-03-28T13:27:45.449606",
     "exception": false,
     "start_time": "2021-03-28T13:27:45.440806",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Again, we consider data from the U.S. March Supplement of the Current Population Survey (CPS) in 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This notebook contains an example for teaching.\n",
    "\n",
    "\n",
    "# A Simple Case Study using Wage Data from 2015 - proceeding\n",
    "\n",
    "So far we considered many machine learning method, e.g Lasso and Random Forests, to build a predictive model. In this lab, we extend our toolbox by predicting wages by a neural network.\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "Again, we consider data from the U.S. March Supplement of the Current Population Survey (CPS) in 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr\n",
    "from sklearn import preprocessing\n",
    "import patsy\n",
    "\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdata_read = pyreadr.read_r(\"../data/wage2015_subsample_inference.Rdata\")\n",
    "data = rdata_read[ 'data' ]\n",
    "n = data.shape[0]\n",
    "\n",
    "type(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2732, 2607, 1653, ..., 4184, 2349, 3462])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import relevant packages for splitting data\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "# Set Seed\n",
    "# to make the results replicable (generating random numbers)\n",
    "np.random.seed(0)\n",
    "random = np.random.randint(0, data.shape[0], size=math.floor(data.shape[0]))\n",
    "data[\"random\"] = random\n",
    "random    # the array does not change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage</th>\n",
       "      <th>lwage</th>\n",
       "      <th>sex</th>\n",
       "      <th>shs</th>\n",
       "      <th>hsg</th>\n",
       "      <th>scl</th>\n",
       "      <th>clg</th>\n",
       "      <th>ad</th>\n",
       "      <th>mw</th>\n",
       "      <th>so</th>\n",
       "      <th>...</th>\n",
       "      <th>ne</th>\n",
       "      <th>exp1</th>\n",
       "      <th>exp2</th>\n",
       "      <th>exp3</th>\n",
       "      <th>exp4</th>\n",
       "      <th>occ</th>\n",
       "      <th>occ2</th>\n",
       "      <th>ind</th>\n",
       "      <th>ind2</th>\n",
       "      <th>random</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rownames</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>26.442308</td>\n",
       "      <td>3.274965</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>8.4100</td>\n",
       "      <td>24.389000</td>\n",
       "      <td>70.728100</td>\n",
       "      <td>340</td>\n",
       "      <td>1</td>\n",
       "      <td>8660</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3467</th>\n",
       "      <td>19.230769</td>\n",
       "      <td>2.956512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.5</td>\n",
       "      <td>11.2225</td>\n",
       "      <td>37.595375</td>\n",
       "      <td>125.944506</td>\n",
       "      <td>9620</td>\n",
       "      <td>22</td>\n",
       "      <td>1870</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13501</th>\n",
       "      <td>48.076923</td>\n",
       "      <td>3.872802</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>3060</td>\n",
       "      <td>10</td>\n",
       "      <td>8190</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15588</th>\n",
       "      <td>12.019231</td>\n",
       "      <td>2.486508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>8.4100</td>\n",
       "      <td>24.389000</td>\n",
       "      <td>70.728100</td>\n",
       "      <td>6440</td>\n",
       "      <td>19</td>\n",
       "      <td>770</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16049</th>\n",
       "      <td>39.903846</td>\n",
       "      <td>3.686473</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.4400</td>\n",
       "      <td>1.728000</td>\n",
       "      <td>2.073600</td>\n",
       "      <td>1820</td>\n",
       "      <td>5</td>\n",
       "      <td>7860</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               wage     lwage  sex  shs  hsg  scl  clg   ad   mw   so  ...  \\\n",
       "rownames                                                               ...   \n",
       "2223      26.442308  3.274965  1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3467      19.230769  2.956512  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...   \n",
       "13501     48.076923  3.872802  1.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  ...   \n",
       "15588     12.019231  2.486508  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  ...   \n",
       "16049     39.903846  3.686473  1.0  0.0  0.0  0.0  0.0  1.0  0.0  1.0  ...   \n",
       "\n",
       "           ne  exp1     exp2       exp3        exp4   occ occ2   ind ind2  \\\n",
       "rownames                                                                    \n",
       "2223      1.0  29.0   8.4100  24.389000   70.728100   340    1  8660   20   \n",
       "3467      1.0  33.5  11.2225  37.595375  125.944506  9620   22  1870    5   \n",
       "13501     0.0   2.0   0.0400   0.008000    0.001600  3060   10  8190   18   \n",
       "15588     0.0  29.0   8.4100  24.389000   70.728100  6440   19   770    4   \n",
       "16049     0.0  12.0   1.4400   1.728000    2.073600  1820    5  7860   17   \n",
       "\n",
       "         random  \n",
       "rownames         \n",
       "2223          0  \n",
       "3467          0  \n",
       "13501         0  \n",
       "15588         2  \n",
       "16049         2  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2 = data.sort_values(by=['random'])\n",
    "data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3862, 21)\n",
      "(1288, 21)\n"
     ]
    }
   ],
   "source": [
    "# Create training and testing sample \n",
    "data_train = data_2[ : math.floor(n*3/4)]    # training sample\n",
    "data_test =  data_2[ math.floor(n*3/4) : ]   # testing sample\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage</th>\n",
       "      <th>lwage</th>\n",
       "      <th>sex</th>\n",
       "      <th>shs</th>\n",
       "      <th>hsg</th>\n",
       "      <th>scl</th>\n",
       "      <th>clg</th>\n",
       "      <th>ad</th>\n",
       "      <th>mw</th>\n",
       "      <th>so</th>\n",
       "      <th>we</th>\n",
       "      <th>ne</th>\n",
       "      <th>exp1</th>\n",
       "      <th>exp2</th>\n",
       "      <th>exp3</th>\n",
       "      <th>exp4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rownames</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9210</th>\n",
       "      <td>29.807692</td>\n",
       "      <td>3.394766</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.84</td>\n",
       "      <td>10.648</td>\n",
       "      <td>23.4256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16484</th>\n",
       "      <td>43.269231</td>\n",
       "      <td>3.767442</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.89</td>\n",
       "      <td>4.913</td>\n",
       "      <td>8.3521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16448</th>\n",
       "      <td>24.038462</td>\n",
       "      <td>3.179655</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1.331</td>\n",
       "      <td>1.4641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27392</th>\n",
       "      <td>10.097115</td>\n",
       "      <td>2.312250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>27.000</td>\n",
       "      <td>81.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11596</th>\n",
       "      <td>8.653846</td>\n",
       "      <td>2.158004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.1296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27533</th>\n",
       "      <td>21.634615</td>\n",
       "      <td>3.074295</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7218</th>\n",
       "      <td>13.461538</td>\n",
       "      <td>2.599837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>7.84</td>\n",
       "      <td>21.952</td>\n",
       "      <td>61.4656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7204</th>\n",
       "      <td>27.403846</td>\n",
       "      <td>3.310683</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.0256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>16.695804</td>\n",
       "      <td>2.815157</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>7.84</td>\n",
       "      <td>21.952</td>\n",
       "      <td>61.4656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10451</th>\n",
       "      <td>45.528846</td>\n",
       "      <td>3.818346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1.331</td>\n",
       "      <td>1.4641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1288 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               wage     lwage  sex  shs  hsg  scl  clg   ad   mw   so   we  \\\n",
       "rownames                                                                     \n",
       "9210      29.807692  3.394766  1.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "16484     43.269231  3.767442  1.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0   \n",
       "16448     24.038462  3.179655  1.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0   \n",
       "27392     10.097115  2.312250  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
       "11596      8.653846  2.158004  1.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "...             ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "27533     21.634615  3.074295  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0   \n",
       "7218      13.461538  2.599837  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "7204      27.403846  3.310683  1.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0   \n",
       "1380      16.695804  2.815157  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "10451     45.528846  3.818346  1.0  0.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0   \n",
       "\n",
       "           ne  exp1  exp2    exp3     exp4  \n",
       "rownames                                    \n",
       "9210      0.0  22.0  4.84  10.648  23.4256  \n",
       "16484     0.0  17.0  2.89   4.913   8.3521  \n",
       "16448     0.0  11.0  1.21   1.331   1.4641  \n",
       "27392     0.0  30.0  9.00  27.000  81.0000  \n",
       "11596     0.0   6.0  0.36   0.216   0.1296  \n",
       "...       ...   ...   ...     ...      ...  \n",
       "27533     0.0  10.0  1.00   1.000   1.0000  \n",
       "7218      0.0  28.0  7.84  21.952  61.4656  \n",
       "7204      0.0   4.0  0.16   0.064   0.0256  \n",
       "1380      1.0  28.0  7.84  21.952  61.4656  \n",
       "10451     0.0  11.0  1.21   1.331   1.4641  \n",
       "\n",
       "[1288 rows x 16 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = data_train.iloc[:, 0:16]\n",
    "data_test = data_test.iloc[:, 0:16] \n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(data_train)\n",
    "scaler = preprocessing.StandardScaler().fit(data_test)\n",
    "\n",
    "data_train_scaled = scaler.transform(data_train)\n",
    "data_test_scaled = scaler.transform(data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage</th>\n",
       "      <th>lwage</th>\n",
       "      <th>sex</th>\n",
       "      <th>shs</th>\n",
       "      <th>hsg</th>\n",
       "      <th>scl</th>\n",
       "      <th>clg</th>\n",
       "      <th>ad</th>\n",
       "      <th>mw</th>\n",
       "      <th>so</th>\n",
       "      <th>we</th>\n",
       "      <th>ne</th>\n",
       "      <th>exp1</th>\n",
       "      <th>exp2</th>\n",
       "      <th>exp3</th>\n",
       "      <th>exp4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.333677</td>\n",
       "      <td>0.755193</td>\n",
       "      <td>1.134788</td>\n",
       "      <td>-0.15177</td>\n",
       "      <td>1.806904</td>\n",
       "      <td>-0.620440</td>\n",
       "      <td>-0.706695</td>\n",
       "      <td>-0.389945</td>\n",
       "      <td>1.633865</td>\n",
       "      <td>-0.640885</td>\n",
       "      <td>-0.527046</td>\n",
       "      <td>-0.529451</td>\n",
       "      <td>0.749646</td>\n",
       "      <td>0.431021</td>\n",
       "      <td>0.147658</td>\n",
       "      <td>-0.046012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.018171</td>\n",
       "      <td>1.425444</td>\n",
       "      <td>1.134788</td>\n",
       "      <td>-0.15177</td>\n",
       "      <td>-0.553433</td>\n",
       "      <td>-0.620440</td>\n",
       "      <td>1.415037</td>\n",
       "      <td>-0.389945</td>\n",
       "      <td>-0.612046</td>\n",
       "      <td>1.560342</td>\n",
       "      <td>-0.527046</td>\n",
       "      <td>-0.529451</td>\n",
       "      <td>0.279612</td>\n",
       "      <td>-0.051673</td>\n",
       "      <td>-0.243088</td>\n",
       "      <td>-0.323785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.040322</td>\n",
       "      <td>0.368319</td>\n",
       "      <td>1.134788</td>\n",
       "      <td>-0.15177</td>\n",
       "      <td>-0.553433</td>\n",
       "      <td>-0.620440</td>\n",
       "      <td>1.415037</td>\n",
       "      <td>-0.389945</td>\n",
       "      <td>-0.612046</td>\n",
       "      <td>1.560342</td>\n",
       "      <td>-0.527046</td>\n",
       "      <td>-0.529451</td>\n",
       "      <td>-0.284429</td>\n",
       "      <td>-0.467532</td>\n",
       "      <td>-0.487143</td>\n",
       "      <td>-0.450716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.668570</td>\n",
       "      <td>-1.191697</td>\n",
       "      <td>-0.881222</td>\n",
       "      <td>-0.15177</td>\n",
       "      <td>1.806904</td>\n",
       "      <td>-0.620440</td>\n",
       "      <td>-0.706695</td>\n",
       "      <td>-0.389945</td>\n",
       "      <td>-0.612046</td>\n",
       "      <td>-0.640885</td>\n",
       "      <td>1.897367</td>\n",
       "      <td>-0.529451</td>\n",
       "      <td>1.501701</td>\n",
       "      <td>1.460768</td>\n",
       "      <td>1.261777</td>\n",
       "      <td>1.014963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.741957</td>\n",
       "      <td>-1.469106</td>\n",
       "      <td>1.134788</td>\n",
       "      <td>-0.15177</td>\n",
       "      <td>1.806904</td>\n",
       "      <td>-0.620440</td>\n",
       "      <td>-0.706695</td>\n",
       "      <td>-0.389945</td>\n",
       "      <td>1.633865</td>\n",
       "      <td>-0.640885</td>\n",
       "      <td>-0.527046</td>\n",
       "      <td>-0.529451</td>\n",
       "      <td>-0.754463</td>\n",
       "      <td>-0.677937</td>\n",
       "      <td>-0.563112</td>\n",
       "      <td>-0.475308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>-0.081909</td>\n",
       "      <td>0.178829</td>\n",
       "      <td>1.134788</td>\n",
       "      <td>-0.15177</td>\n",
       "      <td>-0.553433</td>\n",
       "      <td>-0.620440</td>\n",
       "      <td>1.415037</td>\n",
       "      <td>-0.389945</td>\n",
       "      <td>-0.612046</td>\n",
       "      <td>-0.640885</td>\n",
       "      <td>1.897367</td>\n",
       "      <td>-0.529451</td>\n",
       "      <td>-0.378436</td>\n",
       "      <td>-0.519515</td>\n",
       "      <td>-0.509695</td>\n",
       "      <td>-0.459268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>-0.497495</td>\n",
       "      <td>-0.674476</td>\n",
       "      <td>-0.881222</td>\n",
       "      <td>-0.15177</td>\n",
       "      <td>1.806904</td>\n",
       "      <td>-0.620440</td>\n",
       "      <td>-0.706695</td>\n",
       "      <td>-0.389945</td>\n",
       "      <td>1.633865</td>\n",
       "      <td>-0.640885</td>\n",
       "      <td>-0.527046</td>\n",
       "      <td>-0.529451</td>\n",
       "      <td>1.313687</td>\n",
       "      <td>1.173627</td>\n",
       "      <td>0.917839</td>\n",
       "      <td>0.654985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>0.211446</td>\n",
       "      <td>0.603971</td>\n",
       "      <td>1.134788</td>\n",
       "      <td>-0.15177</td>\n",
       "      <td>-0.553433</td>\n",
       "      <td>-0.620440</td>\n",
       "      <td>-0.706695</td>\n",
       "      <td>2.564463</td>\n",
       "      <td>1.633865</td>\n",
       "      <td>-0.640885</td>\n",
       "      <td>-0.527046</td>\n",
       "      <td>-0.529451</td>\n",
       "      <td>-0.942477</td>\n",
       "      <td>-0.727445</td>\n",
       "      <td>-0.573468</td>\n",
       "      <td>-0.477224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>-0.333039</td>\n",
       "      <td>-0.287225</td>\n",
       "      <td>1.134788</td>\n",
       "      <td>-0.15177</td>\n",
       "      <td>-0.553433</td>\n",
       "      <td>1.611758</td>\n",
       "      <td>-0.706695</td>\n",
       "      <td>-0.389945</td>\n",
       "      <td>-0.612046</td>\n",
       "      <td>-0.640885</td>\n",
       "      <td>-0.527046</td>\n",
       "      <td>1.888750</td>\n",
       "      <td>1.313687</td>\n",
       "      <td>1.173627</td>\n",
       "      <td>0.917839</td>\n",
       "      <td>0.654985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>1.133069</td>\n",
       "      <td>1.516995</td>\n",
       "      <td>1.134788</td>\n",
       "      <td>-0.15177</td>\n",
       "      <td>-0.553433</td>\n",
       "      <td>-0.620440</td>\n",
       "      <td>1.415037</td>\n",
       "      <td>-0.389945</td>\n",
       "      <td>1.633865</td>\n",
       "      <td>-0.640885</td>\n",
       "      <td>-0.527046</td>\n",
       "      <td>-0.529451</td>\n",
       "      <td>-0.284429</td>\n",
       "      <td>-0.467532</td>\n",
       "      <td>-0.487143</td>\n",
       "      <td>-0.450716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1288 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          wage     lwage       sex      shs       hsg       scl       clg  \\\n",
       "0     0.333677  0.755193  1.134788 -0.15177  1.806904 -0.620440 -0.706695   \n",
       "1     1.018171  1.425444  1.134788 -0.15177 -0.553433 -0.620440  1.415037   \n",
       "2     0.040322  0.368319  1.134788 -0.15177 -0.553433 -0.620440  1.415037   \n",
       "3    -0.668570 -1.191697 -0.881222 -0.15177  1.806904 -0.620440 -0.706695   \n",
       "4    -0.741957 -1.469106  1.134788 -0.15177  1.806904 -0.620440 -0.706695   \n",
       "...        ...       ...       ...      ...       ...       ...       ...   \n",
       "1283 -0.081909  0.178829  1.134788 -0.15177 -0.553433 -0.620440  1.415037   \n",
       "1284 -0.497495 -0.674476 -0.881222 -0.15177  1.806904 -0.620440 -0.706695   \n",
       "1285  0.211446  0.603971  1.134788 -0.15177 -0.553433 -0.620440 -0.706695   \n",
       "1286 -0.333039 -0.287225  1.134788 -0.15177 -0.553433  1.611758 -0.706695   \n",
       "1287  1.133069  1.516995  1.134788 -0.15177 -0.553433 -0.620440  1.415037   \n",
       "\n",
       "            ad        mw        so        we        ne      exp1      exp2  \\\n",
       "0    -0.389945  1.633865 -0.640885 -0.527046 -0.529451  0.749646  0.431021   \n",
       "1    -0.389945 -0.612046  1.560342 -0.527046 -0.529451  0.279612 -0.051673   \n",
       "2    -0.389945 -0.612046  1.560342 -0.527046 -0.529451 -0.284429 -0.467532   \n",
       "3    -0.389945 -0.612046 -0.640885  1.897367 -0.529451  1.501701  1.460768   \n",
       "4    -0.389945  1.633865 -0.640885 -0.527046 -0.529451 -0.754463 -0.677937   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1283 -0.389945 -0.612046 -0.640885  1.897367 -0.529451 -0.378436 -0.519515   \n",
       "1284 -0.389945  1.633865 -0.640885 -0.527046 -0.529451  1.313687  1.173627   \n",
       "1285  2.564463  1.633865 -0.640885 -0.527046 -0.529451 -0.942477 -0.727445   \n",
       "1286 -0.389945 -0.612046 -0.640885 -0.527046  1.888750  1.313687  1.173627   \n",
       "1287 -0.389945  1.633865 -0.640885 -0.527046 -0.529451 -0.284429 -0.467532   \n",
       "\n",
       "          exp3      exp4  \n",
       "0     0.147658 -0.046012  \n",
       "1    -0.243088 -0.323785  \n",
       "2    -0.487143 -0.450716  \n",
       "3     1.261777  1.014963  \n",
       "4    -0.563112 -0.475308  \n",
       "...        ...       ...  \n",
       "1283 -0.509695 -0.459268  \n",
       "1284  0.917839  0.654985  \n",
       "1285 -0.573468 -0.477224  \n",
       "1286  0.917839  0.654985  \n",
       "1287 -0.487143 -0.450716  \n",
       "\n",
       "[1288 rows x 16 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_scaled = pd.DataFrame(data_train_scaled, columns = columns)\n",
    "data_test_scaled = pd.DataFrame(data_test_scaled, columns = columns)\n",
    "data_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we construct the inputs for our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_basic = \"lwage ~ sex + exp1 + shs + hsg+ scl + clg + mw + so + we\"\n",
    "Y_train, model_X_basic_train = patsy.dmatrices(formula_basic, data_train_scaled, return_type='dataframe')\n",
    "Y_test, model_X_basic_test = patsy.dmatrices(formula_basic, data_test_scaled, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to determine the structure of our network. We are using the R package *keras* to build a simple sequential neural network with three dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim = model_X_basic_train.shape[1], activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "opt = keras.optimizers.Adam(learning_rate=0.005)\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "mae = tf.keras.metrics.MeanAbsoluteError(name=\"mean_absolute_error\", dtype=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the structure of our network in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 441\n",
      "Trainable params: 441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=mse, optimizer= opt , metrics=mae)\n",
    "model.summary(line_length=None, positions=None, print_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth to notice that we have in total $441$ trainable parameters.\n",
    "\n",
    "Now, let us train the network. Note that this takes some computation time. Thus, we are using gpu to speed up. The exact speed-up varies based on a number of factors including model architecture, batch-size, input pipeline complexity, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the keras model on the dataset\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "  1/387 [..............................] - ETA: 0s - loss: 0.8985 - mean_absolute_error: 0.7327WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "387/387 [==============================] - 0s 781us/step - loss: 0.9216 - mean_absolute_error: 0.7322\n",
      "Epoch 2/150\n",
      "387/387 [==============================] - 0s 711us/step - loss: 0.8893 - mean_absolute_error: 0.7221\n",
      "Epoch 3/150\n",
      "387/387 [==============================] - 0s 475us/step - loss: 0.8742 - mean_absolute_error: 0.7139\n",
      "Epoch 4/150\n",
      "387/387 [==============================] - 0s 480us/step - loss: 0.8752 - mean_absolute_error: 0.7134\n",
      "Epoch 5/150\n",
      "387/387 [==============================] - 0s 473us/step - loss: 0.8657 - mean_absolute_error: 0.7067\n",
      "Epoch 6/150\n",
      "387/387 [==============================] - 0s 472us/step - loss: 0.8677 - mean_absolute_error: 0.7106\n",
      "Epoch 7/150\n",
      "387/387 [==============================] - 0s 507us/step - loss: 0.8627 - mean_absolute_error: 0.7082\n",
      "Epoch 8/150\n",
      "387/387 [==============================] - 0s 454us/step - loss: 0.8623 - mean_absolute_error: 0.7078\n",
      "Epoch 9/150\n",
      "387/387 [==============================] - 0s 433us/step - loss: 0.8554 - mean_absolute_error: 0.7064\n",
      "Epoch 10/150\n",
      "387/387 [==============================] - 0s 425us/step - loss: 0.8566 - mean_absolute_error: 0.7058\n",
      "Epoch 11/150\n",
      "387/387 [==============================] - 0s 415us/step - loss: 0.8558 - mean_absolute_error: 0.7057\n",
      "Epoch 12/150\n",
      "387/387 [==============================] - 0s 457us/step - loss: 0.8531 - mean_absolute_error: 0.7057\n",
      "Epoch 13/150\n",
      "387/387 [==============================] - 0s 423us/step - loss: 0.8549 - mean_absolute_error: 0.7044\n",
      "Epoch 14/150\n",
      "387/387 [==============================] - 0s 418us/step - loss: 0.8479 - mean_absolute_error: 0.7040\n",
      "Epoch 15/150\n",
      "387/387 [==============================] - 0s 428us/step - loss: 0.8511 - mean_absolute_error: 0.7030\n",
      "Epoch 16/150\n",
      "387/387 [==============================] - 0s 433us/step - loss: 0.8465 - mean_absolute_error: 0.7011\n",
      "Epoch 17/150\n",
      "387/387 [==============================] - 0s 441us/step - loss: 0.8472 - mean_absolute_error: 0.7025\n",
      "Epoch 18/150\n",
      "387/387 [==============================] - 0s 451us/step - loss: 0.8485 - mean_absolute_error: 0.7017\n",
      "Epoch 19/150\n",
      "387/387 [==============================] - 0s 459us/step - loss: 0.8453 - mean_absolute_error: 0.6996\n",
      "Epoch 20/150\n",
      "387/387 [==============================] - 0s 456us/step - loss: 0.8395 - mean_absolute_error: 0.7001\n",
      "Epoch 21/150\n",
      "387/387 [==============================] - 0s 484us/step - loss: 0.8459 - mean_absolute_error: 0.7008\n",
      "Epoch 22/150\n",
      "387/387 [==============================] - 0s 493us/step - loss: 0.8449 - mean_absolute_error: 0.7002\n",
      "Epoch 23/150\n",
      "387/387 [==============================] - 0s 450us/step - loss: 0.8449 - mean_absolute_error: 0.6997\n",
      "Epoch 24/150\n",
      "387/387 [==============================] - 0s 438us/step - loss: 0.8440 - mean_absolute_error: 0.7015\n",
      "Epoch 25/150\n",
      "387/387 [==============================] - 0s 415us/step - loss: 0.8410 - mean_absolute_error: 0.6988\n",
      "Epoch 26/150\n",
      "387/387 [==============================] - 0s 428us/step - loss: 0.8408 - mean_absolute_error: 0.6975\n",
      "Epoch 27/150\n",
      "387/387 [==============================] - 0s 415us/step - loss: 0.8451 - mean_absolute_error: 0.7006\n",
      "Epoch 28/150\n",
      "387/387 [==============================] - 0s 425us/step - loss: 0.8376 - mean_absolute_error: 0.6978\n",
      "Epoch 29/150\n",
      "387/387 [==============================] - 0s 428us/step - loss: 0.8380 - mean_absolute_error: 0.6977\n",
      "Epoch 30/150\n",
      "387/387 [==============================] - 0s 412us/step - loss: 0.8395 - mean_absolute_error: 0.7005\n",
      "Epoch 31/150\n",
      "387/387 [==============================] - 0s 428us/step - loss: 0.8408 - mean_absolute_error: 0.6991\n",
      "Epoch 32/150\n",
      "387/387 [==============================] - 0s 425us/step - loss: 0.8404 - mean_absolute_error: 0.6991\n",
      "Epoch 33/150\n",
      "387/387 [==============================] - 0s 425us/step - loss: 0.8391 - mean_absolute_error: 0.6985\n",
      "Epoch 34/150\n",
      "387/387 [==============================] - 0s 465us/step - loss: 0.8384 - mean_absolute_error: 0.6953\n",
      "Epoch 35/150\n",
      "387/387 [==============================] - 0s 423us/step - loss: 0.8343 - mean_absolute_error: 0.6989\n",
      "Epoch 36/150\n",
      "387/387 [==============================] - 0s 453us/step - loss: 0.8379 - mean_absolute_error: 0.6992\n",
      "Epoch 37/150\n",
      "387/387 [==============================] - 0s 456us/step - loss: 0.8365 - mean_absolute_error: 0.6991\n",
      "Epoch 38/150\n",
      "387/387 [==============================] - 0s 482us/step - loss: 0.8337 - mean_absolute_error: 0.6967\n",
      "Epoch 39/150\n",
      "387/387 [==============================] - 0s 461us/step - loss: 0.8335 - mean_absolute_error: 0.6962\n",
      "Epoch 40/150\n",
      "387/387 [==============================] - 0s 448us/step - loss: 0.8354 - mean_absolute_error: 0.6989\n",
      "Epoch 41/150\n",
      "387/387 [==============================] - 0s 427us/step - loss: 0.8361 - mean_absolute_error: 0.6975\n",
      "Epoch 42/150\n",
      "387/387 [==============================] - 0s 419us/step - loss: 0.8360 - mean_absolute_error: 0.6999\n",
      "Epoch 43/150\n",
      "387/387 [==============================] - 0s 443us/step - loss: 0.8376 - mean_absolute_error: 0.6985\n",
      "Epoch 44/150\n",
      "387/387 [==============================] - 0s 442us/step - loss: 0.8374 - mean_absolute_error: 0.6984\n",
      "Epoch 45/150\n",
      "387/387 [==============================] - 0s 526us/step - loss: 0.8340 - mean_absolute_error: 0.6970\n",
      "Epoch 46/150\n",
      "387/387 [==============================] - 0s 483us/step - loss: 0.8302 - mean_absolute_error: 0.6947\n",
      "Epoch 47/150\n",
      "387/387 [==============================] - 0s 454us/step - loss: 0.8355 - mean_absolute_error: 0.6964\n",
      "Epoch 48/150\n",
      "387/387 [==============================] - 0s 428us/step - loss: 0.8313 - mean_absolute_error: 0.6969\n",
      "Epoch 49/150\n",
      "387/387 [==============================] - 0s 431us/step - loss: 0.8341 - mean_absolute_error: 0.6972\n",
      "Epoch 50/150\n",
      "387/387 [==============================] - 0s 427us/step - loss: 0.8309 - mean_absolute_error: 0.6960\n",
      "Epoch 51/150\n",
      "387/387 [==============================] - 0s 435us/step - loss: 0.8378 - mean_absolute_error: 0.6986\n",
      "Epoch 52/150\n",
      "387/387 [==============================] - 0s 484us/step - loss: 0.8333 - mean_absolute_error: 0.6976\n",
      "Epoch 53/150\n",
      "387/387 [==============================] - 0s 430us/step - loss: 0.8313 - mean_absolute_error: 0.6930\n",
      "Epoch 54/150\n",
      "387/387 [==============================] - 0s 433us/step - loss: 0.8310 - mean_absolute_error: 0.6946\n",
      "Epoch 55/150\n",
      "387/387 [==============================] - 0s 412us/step - loss: 0.8304 - mean_absolute_error: 0.6961\n",
      "Epoch 56/150\n",
      "387/387 [==============================] - 0s 417us/step - loss: 0.8325 - mean_absolute_error: 0.6956\n",
      "Epoch 57/150\n",
      "387/387 [==============================] - 0s 411us/step - loss: 0.8291 - mean_absolute_error: 0.6958\n",
      "Epoch 58/150\n",
      "387/387 [==============================] - 0s 447us/step - loss: 0.8344 - mean_absolute_error: 0.6970\n",
      "Epoch 59/150\n",
      "387/387 [==============================] - 0s 502us/step - loss: 0.8331 - mean_absolute_error: 0.6976\n",
      "Epoch 60/150\n",
      "387/387 [==============================] - 0s 501us/step - loss: 0.8327 - mean_absolute_error: 0.6955\n",
      "Epoch 61/150\n",
      "387/387 [==============================] - 0s 487us/step - loss: 0.8290 - mean_absolute_error: 0.6960\n",
      "Epoch 62/150\n",
      "387/387 [==============================] - 0s 430us/step - loss: 0.8309 - mean_absolute_error: 0.6963\n",
      "Epoch 63/150\n",
      "387/387 [==============================] - 0s 423us/step - loss: 0.8267 - mean_absolute_error: 0.6946\n",
      "Epoch 64/150\n",
      "387/387 [==============================] - 0s 428us/step - loss: 0.8267 - mean_absolute_error: 0.6932\n",
      "Epoch 65/150\n",
      "387/387 [==============================] - 0s 461us/step - loss: 0.8296 - mean_absolute_error: 0.6939\n",
      "Epoch 66/150\n",
      "387/387 [==============================] - 0s 412us/step - loss: 0.8312 - mean_absolute_error: 0.6963\n",
      "Epoch 67/150\n",
      "387/387 [==============================] - 0s 414us/step - loss: 0.8320 - mean_absolute_error: 0.6958\n",
      "Epoch 68/150\n",
      "387/387 [==============================] - 0s 474us/step - loss: 0.8288 - mean_absolute_error: 0.6939\n",
      "Epoch 69/150\n",
      "387/387 [==============================] - 0s 487us/step - loss: 0.8314 - mean_absolute_error: 0.6965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/150\n",
      "387/387 [==============================] - 0s 472us/step - loss: 0.8259 - mean_absolute_error: 0.6925\n",
      "Epoch 71/150\n",
      "387/387 [==============================] - 0s 430us/step - loss: 0.8276 - mean_absolute_error: 0.6923\n",
      "Epoch 72/150\n",
      "387/387 [==============================] - 0s 461us/step - loss: 0.8256 - mean_absolute_error: 0.6939\n",
      "Epoch 73/150\n",
      "387/387 [==============================] - 0s 423us/step - loss: 0.8270 - mean_absolute_error: 0.6928\n",
      "Epoch 74/150\n",
      "387/387 [==============================] - 0s 425us/step - loss: 0.8258 - mean_absolute_error: 0.6923\n",
      "Epoch 75/150\n",
      "387/387 [==============================] - 0s 487us/step - loss: 0.8297 - mean_absolute_error: 0.6941\n",
      "Epoch 76/150\n",
      "387/387 [==============================] - 0s 423us/step - loss: 0.8250 - mean_absolute_error: 0.6912\n",
      "Epoch 77/150\n",
      "387/387 [==============================] - 0s 425us/step - loss: 0.8258 - mean_absolute_error: 0.6933\n",
      "Epoch 78/150\n",
      "387/387 [==============================] - 0s 433us/step - loss: 0.8314 - mean_absolute_error: 0.6951\n",
      "Epoch 79/150\n",
      "387/387 [==============================] - 0s 412us/step - loss: 0.8266 - mean_absolute_error: 0.6940\n",
      "Epoch 80/150\n",
      "387/387 [==============================] - 0s 423us/step - loss: 0.8276 - mean_absolute_error: 0.6953\n",
      "Epoch 81/150\n",
      "387/387 [==============================] - 0s 454us/step - loss: 0.8247 - mean_absolute_error: 0.6923\n",
      "Epoch 82/150\n",
      "387/387 [==============================] - 0s 415us/step - loss: 0.8249 - mean_absolute_error: 0.6928\n",
      "Epoch 83/150\n",
      "387/387 [==============================] - 0s 423us/step - loss: 0.8251 - mean_absolute_error: 0.6910\n",
      "Epoch 84/150\n",
      "387/387 [==============================] - 0s 427us/step - loss: 0.8306 - mean_absolute_error: 0.6957\n",
      "Epoch 85/150\n",
      "387/387 [==============================] - 0s 423us/step - loss: 0.8249 - mean_absolute_error: 0.6923\n",
      "Epoch 86/150\n",
      "387/387 [==============================] - 0s 419us/step - loss: 0.8243 - mean_absolute_error: 0.6920\n",
      "Epoch 87/150\n",
      "387/387 [==============================] - 0s 414us/step - loss: 0.8257 - mean_absolute_error: 0.6940\n",
      "Epoch 88/150\n",
      "387/387 [==============================] - 0s 421us/step - loss: 0.8260 - mean_absolute_error: 0.6935\n",
      "Epoch 89/150\n",
      "387/387 [==============================] - 0s 415us/step - loss: 0.8230 - mean_absolute_error: 0.6922\n",
      "Epoch 90/150\n",
      "387/387 [==============================] - 0s 417us/step - loss: 0.8224 - mean_absolute_error: 0.6899\n",
      "Epoch 91/150\n",
      "387/387 [==============================] - 0s 430us/step - loss: 0.8239 - mean_absolute_error: 0.6911\n",
      "Epoch 92/150\n",
      "387/387 [==============================] - 0s 412us/step - loss: 0.8245 - mean_absolute_error: 0.6918\n",
      "Epoch 93/150\n",
      "387/387 [==============================] - 0s 420us/step - loss: 0.8262 - mean_absolute_error: 0.6935\n",
      "Epoch 94/150\n",
      "387/387 [==============================] - 0s 423us/step - loss: 0.8228 - mean_absolute_error: 0.6931\n",
      "Epoch 95/150\n",
      "387/387 [==============================] - 0s 412us/step - loss: 0.8244 - mean_absolute_error: 0.6920\n",
      "Epoch 96/150\n",
      "387/387 [==============================] - 0s 417us/step - loss: 0.8251 - mean_absolute_error: 0.6920\n",
      "Epoch 97/150\n",
      "387/387 [==============================] - 0s 422us/step - loss: 0.8196 - mean_absolute_error: 0.6908\n",
      "Epoch 98/150\n",
      "387/387 [==============================] - 0s 433us/step - loss: 0.8240 - mean_absolute_error: 0.6935\n",
      "Epoch 99/150\n",
      "387/387 [==============================] - 0s 420us/step - loss: 0.8229 - mean_absolute_error: 0.6913\n",
      "Epoch 100/150\n",
      "387/387 [==============================] - 0s 425us/step - loss: 0.8218 - mean_absolute_error: 0.6912\n",
      "Epoch 101/150\n",
      "387/387 [==============================] - 0s 420us/step - loss: 0.8237 - mean_absolute_error: 0.6926\n",
      "Epoch 102/150\n",
      "387/387 [==============================] - 0s 423us/step - loss: 0.8225 - mean_absolute_error: 0.6927\n",
      "Epoch 103/150\n",
      "387/387 [==============================] - 0s 422us/step - loss: 0.8200 - mean_absolute_error: 0.6912\n",
      "Epoch 104/150\n",
      "387/387 [==============================] - 0s 440us/step - loss: 0.8217 - mean_absolute_error: 0.6925\n",
      "Epoch 105/150\n",
      "387/387 [==============================] - 0s 417us/step - loss: 0.8211 - mean_absolute_error: 0.6908\n",
      "Epoch 106/150\n",
      "387/387 [==============================] - 0s 421us/step - loss: 0.8241 - mean_absolute_error: 0.6943\n",
      "Epoch 107/150\n",
      "387/387 [==============================] - 0s 425us/step - loss: 0.8217 - mean_absolute_error: 0.6910\n",
      "Epoch 108/150\n",
      "387/387 [==============================] - 0s 417us/step - loss: 0.8218 - mean_absolute_error: 0.6900\n",
      "Epoch 109/150\n",
      "387/387 [==============================] - 0s 428us/step - loss: 0.8208 - mean_absolute_error: 0.6925\n",
      "Epoch 110/150\n",
      "387/387 [==============================] - 0s 425us/step - loss: 0.8227 - mean_absolute_error: 0.6917\n",
      "Epoch 111/150\n",
      "387/387 [==============================] - 0s 420us/step - loss: 0.8203 - mean_absolute_error: 0.6908\n",
      "Epoch 112/150\n",
      "387/387 [==============================] - 0s 430us/step - loss: 0.8190 - mean_absolute_error: 0.6905\n",
      "Epoch 113/150\n",
      "387/387 [==============================] - 0s 428us/step - loss: 0.8209 - mean_absolute_error: 0.6903\n",
      "Epoch 114/150\n",
      "387/387 [==============================] - 0s 420us/step - loss: 0.8201 - mean_absolute_error: 0.6909\n",
      "Epoch 115/150\n",
      "387/387 [==============================] - 0s 447us/step - loss: 0.8192 - mean_absolute_error: 0.6906\n",
      "Epoch 116/150\n",
      "387/387 [==============================] - 0s 430us/step - loss: 0.8186 - mean_absolute_error: 0.6910\n",
      "Epoch 117/150\n",
      "387/387 [==============================] - 0s 454us/step - loss: 0.8198 - mean_absolute_error: 0.6908\n",
      "Epoch 118/150\n",
      "387/387 [==============================] - 0s 423us/step - loss: 0.8227 - mean_absolute_error: 0.6928\n",
      "Epoch 119/150\n",
      "387/387 [==============================] - 0s 430us/step - loss: 0.8226 - mean_absolute_error: 0.6918\n",
      "Epoch 120/150\n",
      "387/387 [==============================] - 0s 433us/step - loss: 0.8187 - mean_absolute_error: 0.6898\n",
      "Epoch 121/150\n",
      "387/387 [==============================] - 0s 417us/step - loss: 0.8201 - mean_absolute_error: 0.6917\n",
      "Epoch 122/150\n",
      "387/387 [==============================] - 0s 421us/step - loss: 0.8211 - mean_absolute_error: 0.6909\n",
      "Epoch 123/150\n",
      "387/387 [==============================] - 0s 510us/step - loss: 0.8177 - mean_absolute_error: 0.6888\n",
      "Epoch 124/150\n",
      "387/387 [==============================] - 0s 479us/step - loss: 0.8198 - mean_absolute_error: 0.6882\n",
      "Epoch 125/150\n",
      "387/387 [==============================] - 0s 474us/step - loss: 0.8174 - mean_absolute_error: 0.6900\n",
      "Epoch 126/150\n",
      "387/387 [==============================] - 0s 451us/step - loss: 0.8173 - mean_absolute_error: 0.6899\n",
      "Epoch 127/150\n",
      "387/387 [==============================] - 0s 419us/step - loss: 0.8174 - mean_absolute_error: 0.6891\n",
      "Epoch 128/150\n",
      "387/387 [==============================] - 0s 428us/step - loss: 0.8223 - mean_absolute_error: 0.6905\n",
      "Epoch 129/150\n",
      "387/387 [==============================] - 0s 417us/step - loss: 0.8189 - mean_absolute_error: 0.6913\n",
      "Epoch 130/150\n",
      "387/387 [==============================] - 0s 430us/step - loss: 0.8170 - mean_absolute_error: 0.6895\n",
      "Epoch 131/150\n",
      "387/387 [==============================] - 0s 423us/step - loss: 0.8181 - mean_absolute_error: 0.6899\n",
      "Epoch 132/150\n",
      "387/387 [==============================] - 0s 430us/step - loss: 0.8168 - mean_absolute_error: 0.6889\n",
      "Epoch 133/150\n",
      "387/387 [==============================] - 0s 438us/step - loss: 0.8211 - mean_absolute_error: 0.6921\n",
      "Epoch 134/150\n",
      "387/387 [==============================] - 0s 418us/step - loss: 0.8206 - mean_absolute_error: 0.6904\n",
      "Epoch 135/150\n",
      "387/387 [==============================] - 0s 425us/step - loss: 0.8191 - mean_absolute_error: 0.6912\n",
      "Epoch 136/150\n",
      "387/387 [==============================] - 0s 422us/step - loss: 0.8216 - mean_absolute_error: 0.6913\n",
      "Epoch 137/150\n",
      "387/387 [==============================] - 0s 452us/step - loss: 0.8175 - mean_absolute_error: 0.6879\n",
      "Epoch 138/150\n",
      "387/387 [==============================] - 0s 469us/step - loss: 0.8190 - mean_absolute_error: 0.6899\n",
      "Epoch 139/150\n",
      "387/387 [==============================] - 0s 466us/step - loss: 0.8216 - mean_absolute_error: 0.6903\n",
      "Epoch 140/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387/387 [==============================] - 0s 441us/step - loss: 0.8158 - mean_absolute_error: 0.6890\n",
      "Epoch 141/150\n",
      "387/387 [==============================] - 0s 423us/step - loss: 0.8189 - mean_absolute_error: 0.6909\n",
      "Epoch 142/150\n",
      "387/387 [==============================] - 0s 428us/step - loss: 0.8201 - mean_absolute_error: 0.6909\n",
      "Epoch 143/150\n",
      "387/387 [==============================] - 0s 427us/step - loss: 0.8170 - mean_absolute_error: 0.6900\n",
      "Epoch 144/150\n",
      "387/387 [==============================] - 0s 424us/step - loss: 0.8184 - mean_absolute_error: 0.6907\n",
      "Epoch 145/150\n",
      "387/387 [==============================] - 0s 425us/step - loss: 0.8161 - mean_absolute_error: 0.6901\n",
      "Epoch 146/150\n",
      "387/387 [==============================] - 0s 439us/step - loss: 0.8170 - mean_absolute_error: 0.6909\n",
      "Epoch 147/150\n",
      "387/387 [==============================] - 0s 459us/step - loss: 0.8180 - mean_absolute_error: 0.6921\n",
      "Epoch 148/150\n",
      "387/387 [==============================] - 0s 430us/step - loss: 0.8143 - mean_absolute_error: 0.6894\n",
      "Epoch 149/150\n",
      "387/387 [==============================] - 0s 464us/step - loss: 0.8230 - mean_absolute_error: 0.6910\n",
      "Epoch 150/150\n",
      "387/387 [==============================] - 0s 451us/step - loss: 0.8164 - mean_absolute_error: 0.6885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d904125b50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the keras model on the dataset\n",
    "model.fit(model_X_basic_train, Y_train, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8773821592330933, 0.7171093821525574]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names\n",
    "model.evaluate(model_X_basic_test, Y_test, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.46916604],\n",
       "       [ 0.37125787],\n",
       "       [ 0.05024599],\n",
       "       ...,\n",
       "       [ 0.20264128],\n",
       "       [ 0.3821144 ],\n",
       "       [ 0.11370236]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_nn = model.predict(model_X_basic_test)\n",
    "pred_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Coef.       0.846993\n",
       "Std.Err.    0.046878\n",
       "Name: const, dtype: float64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resid_basic = (Y_test-pred_nn)**2\n",
    "\n",
    "MSE_nn_basic = sm.OLS( resid_basic , np.ones( resid_basic.shape[0] ) ).fit().summary2().tables[1].iloc[0, 0:2]\n",
    "MSE_nn_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R^2 using NN is equal to, 0.15366423475210123\n"
     ]
    }
   ],
   "source": [
    "R2_nn_basic = 1 - ( MSE_nn_basic[0]/Y_test.var() )\n",
    "print( f\"The R^2 using NN is equal to = {R2_nn_basic[0]}\" ) # MSE NN (basic model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
